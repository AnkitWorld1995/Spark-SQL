{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:22:55.252782Z",
     "start_time": "2023-10-24T13:22:55.092022Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/24 18:52:57 WARN Utils: Your hostname, Sivas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.65.79 instead (on interface en0)\n",
      "23/10/24 18:52:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/24 18:52:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Building a Spark-Session using a spark-Session API.\n",
    "# 1 Spark-Session Per JVM.\n",
    "# Spark Driver Connects with Spark-Executor by creating a Spark Session.\n",
    "spark = SparkSession.builder.appName(\"SPARK_01\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:22:58.477536Z",
     "start_time": "2023-10-24T13:22:55.253305Z"
    }
   },
   "id": "83b6f46f841c5448"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Read the File as a CSV format into the Spark DataFrame by inferring the Schema \n",
    "# And specifying that the file has a Header.\n",
    "df  = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"mnm_dataset.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:23:03.118674Z",
     "start_time": "2023-10-24T13:22:58.481103Z"
    }
   },
   "id": "764e32ca5c0b714c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|State|Color|Count|\n",
      "+-----+-----+-----+\n",
      "|   TX|  Red|   20|\n",
      "|   NV|  Red|   98|\n",
      "|   CO|  Red|   82|\n",
      "|   CO|  Red|   12|\n",
      "|   CO|  Red|   17|\n",
      "+-----+-----+-----+\n"
     ]
    }
   ],
   "source": [
    "# Spark consists of 2 major operation for Data Distribution.\n",
    "# 1. Transformation. -->> It transforms the original Spark DataFrame into New Dataframe with a property of immutability.\n",
    "# 2. Action.         -->> It mutates the most Optimized Dataframe from the data lineage into a new dataframe.\n",
    "\n",
    "# Transform Operation\n",
    "sdf = df.filter(df.Color == \"Red\")\n",
    "\n",
    "# Action Operation\n",
    "sdf.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:23:03.349493Z",
     "start_time": "2023-10-24T13:23:03.125042Z"
    }
   },
   "id": "6b8da1b7a8af3f63"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State| Color|Count|\n",
      "+-----+------+-----+\n",
      "|   NV|  Blue|   66|\n",
      "|   CO|  Blue|   79|\n",
      "|   OR|  Blue|   71|\n",
      "|   WA|Yellow|   93|\n",
      "|   CA|Yellow|   53|\n",
      "|   WA| Green|   60|\n",
      "|   OR| Green|   71|\n",
      "|   TX| Green|   68|\n",
      "|   NV| Green|   59|\n",
      "|   AZ| Brown|   95|\n",
      "|   AZ|  Blue|   75|\n",
      "|   OR| Brown|   72|\n",
      "|   NV|   Red|   98|\n",
      "|   WY|Orange|   45|\n",
      "|   CO|  Blue|   52|\n",
      "|   TX| Brown|   94|\n",
      "|   CO|   Red|   82|\n",
      "|   AZ| Green|   46|\n",
      "|   NV|   Red|   43|\n",
      "|   CO|  Blue|   95|\n",
      "+-----+------+-----+\n"
     ]
    }
   ],
   "source": [
    "# 1. Select State, Color, Count Where Count is greater than 20\n",
    "mm_df =  df.select(\"State\", \"Color\", \"Count\").where(df.Count > 20).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:23:03.575670Z",
     "start_time": "2023-10-24T13:23:03.345959Z"
    }
   },
   "id": "ba379ae25670fd70"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 2. Select State, Color, Count  Group by State, Color aggregated by Count.\n",
    "mm_df_01 = df.select(\"State\", \"Color\", \"Count\").groupBy(\"State\", \"Color\").agg(count(\"Count\").alias(\"Total\")).orderBy(\"Total\", ascending = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:23:03.690913Z",
     "start_time": "2023-10-24T13:23:03.525356Z"
    }
   },
   "id": "e0f6a405480f750f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|WY   |Brown |1532 |\n",
      "|UT   |Green |1591 |\n",
      "|WY   |Orange|1595 |\n",
      "|CA   |Blue  |1603 |\n",
      "|NV   |Red   |1610 |\n",
      "|TX   |Blue  |1614 |\n",
      "|OR   |Yellow|1614 |\n",
      "|OR   |Brown |1621 |\n",
      "|CO   |Red   |1624 |\n",
      "|WA   |Blue  |1625 |\n",
      "|WY   |Yellow|1626 |\n",
      "|UT   |Brown |1631 |\n",
      "|OR   |Green |1634 |\n",
      "|AZ   |Blue  |1636 |\n",
      "|NM   |Blue  |1638 |\n",
      "|TX   |Brown |1641 |\n",
      "|CO   |Orange|1642 |\n",
      "|UT   |Yellow|1645 |\n",
      "|OR   |Red   |1645 |\n",
      "|OR   |Blue  |1646 |\n",
      "|AZ   |Red   |1648 |\n",
      "|TX   |Orange|1652 |\n",
      "|AZ   |Yellow|1654 |\n",
      "|UT   |Blue  |1655 |\n",
      "|CA   |Red   |1656 |\n",
      "|CO   |Brown |1656 |\n",
      "|NV   |Brown |1657 |\n",
      "|CA   |Orange|1657 |\n",
      "|WA   |Orange|1658 |\n",
      "|WA   |Yellow|1663 |\n",
      "|WY   |Blue  |1664 |\n",
      "|NM   |Orange|1665 |\n",
      "|WA   |Brown |1669 |\n",
      "|WY   |Red   |1670 |\n",
      "|WA   |Red   |1671 |\n",
      "|NV   |Blue  |1673 |\n",
      "|NV   |Yellow|1675 |\n",
      "|AZ   |Green |1676 |\n",
      "|UT   |Red   |1680 |\n",
      "|NM   |Green |1682 |\n",
      "|UT   |Orange|1684 |\n",
      "|NM   |Brown |1687 |\n",
      "|NM   |Yellow|1688 |\n",
      "|AZ   |Orange|1689 |\n",
      "|NM   |Red   |1690 |\n",
      "|WY   |Green |1695 |\n",
      "|CO   |Blue  |1695 |\n",
      "|NV   |Green |1698 |\n",
      "|AZ   |Brown |1698 |\n",
      "|TX   |Yellow|1703 |\n",
      "|NV   |Orange|1712 |\n",
      "|CO   |Green |1713 |\n",
      "|CA   |Brown |1718 |\n",
      "|CO   |Yellow|1721 |\n",
      "|CA   |Green |1723 |\n",
      "|TX   |Red   |1725 |\n",
      "|TX   |Green |1737 |\n",
      "|OR   |Orange|1743 |\n",
      "|WA   |Green |1779 |\n",
      "|CA   |Yellow|1807 |\n",
      "+-----+------+-----+\n"
     ]
    }
   ],
   "source": [
    "mm_df_01.show(n=600, truncate=False)\n",
    "\n",
    "# Write Spark Dataframe to Drive (CSV).\n",
    "# mm_df_01.write.csv(\"test_01.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:23:04.553354Z",
     "start_time": "2023-10-24T13:23:03.623387Z"
    }
   },
   "id": "ef608e61f8aff917"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 3. Select State, Color, Count where State = CA Group by State, Color aggregated by Count.\n",
    "mm_df_02 = df.select(\"State\", \"Color\", \"Count\").where(df.State == \"WA\").groupBy(\"State\", \"Color\").agg(count(\"Count\").alias(\"Total\")).orderBy(\"Total\", ascending = True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:26:07.703878Z",
     "start_time": "2023-10-24T13:26:07.665868Z"
    }
   },
   "id": "2dc6136e9c62a218"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|WA   |Blue  |1625 |\n",
      "|WA   |Orange|1658 |\n",
      "|WA   |Yellow|1663 |\n",
      "|WA   |Brown |1669 |\n",
      "|WA   |Red   |1671 |\n",
      "|WA   |Green |1779 |\n",
      "+-----+------+-----+\n"
     ]
    }
   ],
   "source": [
    "mm_df_02.show(n=60, truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:23:05.039619Z",
     "start_time": "2023-10-24T13:23:04.589596Z"
    }
   },
   "id": "26195546a29f8b3"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/24 18:56:10 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`default`.`loan_risks_upload`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 8\u001B[0m\n\u001B[1;32m      3\u001B[0m source_format \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCSV\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      6\u001B[0m mm_df_02\u001B[38;5;241m.\u001B[39msql_ctx\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDROP TABLE IF EXISTS \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m table_name)\n\u001B[0;32m----> 8\u001B[0m mm_df_02\u001B[38;5;241m.\u001B[39msql_ctx\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCREATE TABLE \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m table_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (\u001B[39m\u001B[38;5;124m\"\u001B[39m \\\n\u001B[1;32m      9\u001B[0m                                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloan_id BIGINT, \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m     10\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunded_amnt INT, \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m     11\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpaid_amnt DOUBLE, \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m     12\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maddr_state STRING)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     13\u001B[0m           )\n\u001B[1;32m     15\u001B[0m mm_df_02\u001B[38;5;241m.\u001B[39msql_ctx\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCOPY INTO \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m table_name \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m     16\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m FROM \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m source_data \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m     17\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m FILEFORMAT = \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m source_format\n\u001B[1;32m     18\u001B[0m           )\n\u001B[1;32m     20\u001B[0m loan_risks_upload_data \u001B[38;5;241m=\u001B[39m mm_df_02\u001B[38;5;241m.\u001B[39msql_ctx\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT * FROM \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m table_name)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.11/site-packages/pyspark/sql/context.py:560\u001B[0m, in \u001B[0;36mSQLContext.sql\u001B[0;34m(self, sqlQuery)\u001B[0m\n\u001B[1;32m    544\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msql\u001B[39m(\u001B[38;5;28mself\u001B[39m, sqlQuery: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[1;32m    545\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001B[39;00m\n\u001B[1;32m    546\u001B[0m \n\u001B[1;32m    547\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.0.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    558\u001B[0m \u001B[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001B[39;00m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 560\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession\u001B[38;5;241m.\u001B[39msql(sqlQuery)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1627\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1628\u001B[0m         litArgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoArray(\n\u001B[1;32m   1629\u001B[0m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n\u001B[1;32m   1630\u001B[0m         )\n\u001B[0;32m-> 1631\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39msql(sqlQuery, litArgs), \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1632\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1633\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`default`.`loan_risks_upload`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession as sps\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "table_name = 'default.loan_risks_upload'\n",
    "source_data = 'dbfs:/FileStore/mnm_dataset.csv'\n",
    "source_format = 'CSV'\n",
    "\n",
    "spark = sps.builder.appName(\"DATABRICKS_MMCOUNT_SPARK_JOB\").getOrCreate()\n",
    "\n",
    "spark.sql()\n",
    "# Read the File as a CSV into Spark Dataframe by inferring Schema.\n",
    "# And specifying that the file has Header.\n",
    "#\n",
    "# Example: Run spark_job in DataBricks LakeHouse.\n",
    "# Upload the mnm_dataset.csv in the Databricks DBFS.\n",
    "# Copy the URL of the File in the DBFS in the Read Function.\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mergeSchema\", \"true\").option(\"inferSchema\", \"true\").load(source_data)\n",
    "\n",
    "\n",
    "\n",
    "df.sql_ctx.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
    "\n",
    "df.sql_ctx.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
    "                                              \"state STRING, \" + \\\n",
    "               \"color STRING, \" + \\\n",
    "               \"count STRING)\"\n",
    "               )\n",
    "\n",
    "df.sql_ctx.sql(\"COPY INTO \" + table_name + \\\n",
    "               \" FROM '\" + source_data + \"'\" + \\\n",
    "               \" FILEFORMAT = \" + source_format\n",
    "               )\n",
    "\n",
    "loan_risks_upload_data = df.sql_ctx.sql(\"SELECT * FROM \" + table_name)\n",
    "\n",
    "display(loan_risks_upload_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T13:26:10.825904Z",
     "start_time": "2023-10-24T13:26:10.756813Z"
    }
   },
   "id": "edcc1c1e5c565d4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "681491623f2f279a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
